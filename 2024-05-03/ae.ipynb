{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ResNet Based Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Basic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, mode):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == 'encode':\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.resize = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, output_padding=stride-1)\n",
    "            self.conv2 = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.resize = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=1, stride=stride, output_padding=stride-1)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            self.conv2,\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "            self.resize,\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv(x)\n",
    "\n",
    "        if x.shape != identity.shape:\n",
    "            x += self.shortcut(identity)\n",
    "        else:\n",
    "            x += identity\n",
    "        \n",
    "        x = self.act(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cfgs = [(2, 16), (2, 64), (2,256)]\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 1, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        res_blk = []\n",
    "        in_channels = 16\n",
    "        for (num_blk, out_channels) in self.cfgs:\n",
    "            res_blk.append(BasicBlock(in_channels, out_channels, stride=2, mode=\"encode\"))\n",
    "            \n",
    "            for _ in range(1, num_blk):\n",
    "                res_blk.append(BasicBlock(out_channels, out_channels, stride=1, mode=\"encode\"))\n",
    "            \n",
    "            in_channels = out_channels\n",
    "        self.encode = nn.Sequential(*res_blk)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.encode(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cfgs = [(2,256), (2, 64), (2, 16)]\n",
    "\n",
    "        res_blk = []\n",
    "        in_channels = 256\n",
    "        for (num_blk, out_channels) in self.cfgs:\n",
    "            res_blk.append(BasicBlock(in_channels, out_channels, stride=2, mode=\"decode\"))\n",
    "            \n",
    "            for _ in range(1, num_blk):\n",
    "                res_blk.append(BasicBlock(out_channels, out_channels, stride=1, mode=\"decode\"))\n",
    "            \n",
    "            in_channels = out_channels\n",
    "        self.decode = nn.Sequential(*res_blk)\n",
    "\n",
    "        self.de_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 3, 3, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decode(x)\n",
    "        x = self.de_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        output = self.decoder(latent)\n",
    "\n",
    "        return latent, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNNAutoEncoder()\n",
    "random_input = torch.randn(1, 3, 64, 64)\n",
    "latent, random_output = net(random_input)\n",
    "print(f\"latent shape: {latent.shape}, output shape: {random_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer Based Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        assert img_size % patch_size == 0,\\\n",
    "            f\"img_size({img_size} is not divisable by patch_size({patch_size}))\"\n",
    "\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=16, in_chans=3, embed_dim=768, num_heads=12, num_layers=12):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2, embed_dim))\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, in_chans * patch_size * patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_size = 64\n",
    "        in_chans = 3\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        latent = self.encoder(x)\n",
    "        x = self.decoder(x, latent)\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        x = x.reshape(x.shape[0], in_chans, img_size, img_size)\n",
    "        \n",
    "        return latent, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TransformerAutoEncoder()\n",
    "random_input = torch.randn(1, 3, 64, 64)\n",
    "latent, random_output = net(random_input)\n",
    "print(f\"latent shape: {latent.shape}, output shape: {random_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning with CIFAR10Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Library & HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "EPOCH = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(save_name, train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses)+1), train_losses, label ='Train_Loss', marker ='o')\n",
    "    plt.plot(range(1, len(val_losses)+1), val_losses, label ='Validation_Loss', marker ='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    title = f\"{save_name}_loss\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f'./result/{title}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save images\n",
    "def plot_img(save_name, view_data, decoded_data, epoch):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        ax.axis('off')\n",
    "\n",
    "        if idx < 5:\n",
    "            original_image = view_data[idx].detach().cpu().permute(1, 2, 0).numpy()\n",
    "            print(f\"Original Image - Min: {original_image.min()}, Max: {original_image.max()}\")\n",
    "            original_image = np.clip(original_image, 0, 1)\n",
    "            ax.imshow(original_image)\n",
    "            ax.set_title('Original')\n",
    "        \n",
    "        else:\n",
    "            decoded_image = decoded_data[idx-5].detach().cpu().permute(1, 2, 0).numpy()\n",
    "            print(f\"Decoded Image - Min: {decoded_image.min()}, Max: {decoded_image.max()}\")\n",
    "            decoded_image = np.clip(decoded_image, 0, 1)\n",
    "            ax.imshow(decoded_image)\n",
    "            ax.set_title('Decoded')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./result/epoch_{save_name}/epoch_{epoch}_images.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oxford Dataset\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.uint8),\n",
    "    v2.CenterCrop(size=(IMG_SIZE, IMG_SIZE)),\n",
    "    v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]),\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.uint8),\n",
    "    v2.CenterCrop(size=(IMG_SIZE, IMG_SIZE)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]),\n",
    "])\n",
    "\n",
    "# train_datasets\n",
    "train_dataset = CIFAR10(root = '../../datasets/CIFAR10', download=True, train = True, transform=train_transforms)\n",
    "val_dataset  = CIFAR10(root = '../../datasets/CIFAR10', download=True, train = False, transform=val_transforms)\n",
    "\n",
    "# dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Train & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train code\n",
    "def train(net, train_loader, criterion, optimizer, scaler, device):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    net.train()\n",
    "    for inputs, _ in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                _, outputs = net(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            _, outputs = net(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate code\n",
    "def eval(net, val_loader, criterion, device):\n",
    "    val_loss = 0.0\n",
    "    psnr = 0.0\n",
    "    ssim = 0.0\n",
    "    psnr_metric = torchmetrics.image.PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure().to(device)\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                _, outputs = net(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            psnr += psnr_metric(outputs, inputs).item()\n",
    "            ssim += ssim_metric(outputs, inputs).item()\n",
    "        \n",
    "    val_loss /= len(val_loader)\n",
    "    psnr /= len(val_loader)\n",
    "    ssim /= len(val_loader)\n",
    "    print(f\"Test Loss: {val_loss:.4f}, PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, save_name, train_loader, val_loader, criterion, optimizer, scaler, device):\n",
    "    view_data = next(iter(train_loader))[0][:5]\n",
    "    view_data = view_data.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    os.makedirs(f\"result/epoch_{save_name}\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        train_loss = train(net, train_loader, criterion, optimizer, scaler, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss = eval(net, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCH}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            weights = net.state_dict()\n",
    "            torch.save(weights, f'./pth/{save_name}.pth')\n",
    "        \n",
    "        test_x = view_data\n",
    "        _, decoded_data = net(test_x)\n",
    "\n",
    "        plot_img(view_data, decoded_data, epoch)\n",
    "\n",
    "    plot_loss(save_name, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train CNN Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"cnn_cifar\"\n",
    "net = CNNAutoEncoder().to(device)\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = Adam(net.parameters(), lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(net, save_name, train_loader, val_loader, criterion, optimizer, scaler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Transformer Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"transformer_cifar\"\n",
    "net = TransformerAutoEncoder().to(device)\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = Adam(net.parameters(), lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(net, save_name, train_loader, val_loader, criterion, optimizer, scaler, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
