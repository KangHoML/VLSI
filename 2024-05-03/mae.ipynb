{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Seed 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def setup_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Masked Auto Encoder 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. PatchShuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchShuffle(torch.nn.Module):\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = PatchShuffle(0.75)\n",
    "a = torch.rand(16, 2, 10)\n",
    "b, forward_indexes, backward_indexes = shuffle(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 patch_size=2,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=12,\n",
    "                 num_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "\n",
    "        return features, backward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 patch_size=2,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
    "        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:] # remove global feature\n",
    "\n",
    "        patches = self.head(features)\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T-1:] = 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(2, 3, 32, 32)\n",
    "encoder, decoder = MAE_Encoder(), MAE_Decoder()\n",
    "\n",
    "features, backward_indexes = encoder(img)\n",
    "print(forward_indexes.shape)\n",
    "\n",
    "predicted_img, mask = decoder(features, backward_indexes)\n",
    "print(predicted_img.shape)\n",
    "\n",
    "loss = torch.mean((predicted_img - img) ** 2 * mask / 0.75)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. MAE Based Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 patch_size=2,\n",
    "                 emb_dim=192,\n",
    "                 encoder_layer=12,\n",
    "                 encoder_head=3,\n",
    "                 decoder_layer=4,\n",
    "                 decoder_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, encoder_layer, encoder_head, mask_ratio)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, decoder_layer, decoder_head)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "        return predicted_img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Classification을 위한 VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, encoder : MAE_Encoder, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "        \n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        \n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        logits = self.head(features[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-training MAE with CIFAR10Dataset (Self-Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Library & HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from tqdm.notebook import tqdm\n",
    "from einops import rearrange\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 4096\n",
    "MAX_DEVICE_BATCH_SIZE = 512\n",
    "LR = 1.5e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "MASK_RATIO = 0.75\n",
    "EPOCH = 2000\n",
    "WARMUP = 200\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "setup_seed(SEED)\n",
    "\n",
    "model_path = './pth/mae_pretrained_cifar.pt'\n",
    "\n",
    "batch_size = min(MAX_DEVICE_BATCH_SIZE, BATCH_SIZE)\n",
    "assert BATCH_SIZE % batch_size == 0\n",
    "steps_per_update = BATCH_SIZE // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. CIFAR10Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 Dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10('../../datasets/CIFAR10', train=True, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))\n",
    "val_dataset = torchvision.datasets.CIFAR10('../../datasets/CIFAR10', train=False, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))\n",
    "\n",
    "# 데이터 로더 설정\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)\n",
    "writer = SummaryWriter(os.path.join('logs', 'cifar10', 'mae-pretrain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Network 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAE_ViT(mask_ratio=MASK_RATIO).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_mae(model, dataloader, val_dataset, writer, model_path):\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR * BATCH_SIZE / 256, betas=(0.9, 0.95), weight_decay=WEIGHT_DECAY)\n",
    "    lr_func = lambda epoch: min((epoch + 1) / (WARMUP + 1e-8), 0.5 * (math.cos(epoch / EPOCH * math.pi) + 1))\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)\n",
    "\n",
    "    step_count = 0\n",
    "    optim.zero_grad()\n",
    "\n",
    "    for e in range(EPOCH):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for img, label in tqdm(iter(dataloader)):\n",
    "            step_count += 1\n",
    "\n",
    "            img = img.to(device)\n",
    "            predicted_img, mask = model(img)\n",
    "\n",
    "            loss = torch.mean((predicted_img - img) ** 2 * mask) / MASK_RATIO\n",
    "            loss.backward()\n",
    "\n",
    "            if step_count % steps_per_update == 0:\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        writer.add_scalar('mae_loss', avg_loss, global_step=e)\n",
    "        print(f'In epoch {e}, average traning loss is {avg_loss}.')\n",
    "\n",
    "        ''' visualize the first 16 predicted images on val dataset'''\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_img = torch.stack([val_dataset[i][0] for i in range(16)])\n",
    "            val_img = val_img.to(device)\n",
    "\n",
    "            predicted_val_img, mask = model(val_img)\n",
    "            predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "\n",
    "            img = torch.cat([val_img * (1 - mask), predicted_val_img, val_img], dim=0)\n",
    "            img = rearrange(img, '(v h1 w1) c h w -> c (h1 h) (w1 v w)', w1=2, v=3)\n",
    "\n",
    "            writer.add_image('mae_image', (img + 1) / 2, global_step=e)\n",
    "        \n",
    "        ''' save model '''\n",
    "        torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_mae(model, dataloader, val_dataset, writer, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning MAE with CIFAR10Dataset (Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. Library & HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 128\n",
    "MAX_DEVICE_BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 0.05\n",
    "EPOCH = 100\n",
    "WARMUP = 5\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "setup_seed(SEED)\n",
    "\n",
    "model_path = './pth/mae_classifer_cifar.pt'\n",
    "pretrain_path = './pth/mae_pretrained_cifar.pt'\n",
    "\n",
    "batch_size = min(MAX_DEVICE_BATCH_SIZE, BATCH_SIZE)\n",
    "assert BATCH_SIZE % batch_size == 0\n",
    "steps_per_update = BATCH_SIZE // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation function 정의\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # 이미지 크기를 32x32로 조정\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "    ])\n",
    "    \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "])\n",
    "\n",
    "# 데이터셋 설정\n",
    "train_dataset = torchvision.datasets.CIFAR10('../../datasets/CIFAR10', train=True, download=True, transform=transform_train)\n",
    "val_dataset = torchvision.datasets.CIFAR10('../../datasets/CIFAR10', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "writer = SummaryWriter(os.path.join('logs', 'cifar10', 'mae-pretrain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Network 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(pretrain_path, map_location='cpu')\n",
    "model = model.to(device)\n",
    "writer = SummaryWriter(os.path.join('logs', 'cifar10', 'pretrain-cls'))\n",
    "\n",
    "model = ViT_Classifier(model.encoder, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mae(model, train_dataloader, val_dataloader, writer, model_path):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    acc_fn = lambda logit, label: torch.mean((logit.argmax(dim=-1) == label).float())\n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR * BATCH_SIZE / 256, betas=(0.9, 0.999), weight_decay=WEIGHT_DECAY)\n",
    "    lr_func = lambda epoch: min((epoch + 1) / (WARMUP + 1e-8), 0.5 * (math.cos(epoch / EPOCH * math.pi) + 1))\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    step_count = 0\n",
    "    optim.zero_grad()\n",
    "\n",
    "    for e in range(EPOCH):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        acces = []\n",
    "    \n",
    "        for img, label in tqdm(iter(train_dataloader)):\n",
    "            step_count += 1\n",
    "    \n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            logits = model(img)\n",
    "\n",
    "            loss = loss_fn(logits, label)\n",
    "            acc = acc_fn(logits, label)\n",
    "            loss.backward()\n",
    "            \n",
    "            if step_count % steps_per_update == 0:\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            acces.append(acc.item())\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        avg_train_loss = sum(losses) / len(losses)\n",
    "        avg_train_acc = sum(acces) / len(acces)\n",
    "        print(f'In epoch {e}, average training loss is {avg_train_loss}, average training acc is {avg_train_acc}.')\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            acces = []\n",
    "        \n",
    "            for img, label in tqdm(iter(val_dataloader)):\n",
    "                img = img.to(device)\n",
    "                label = label.to(device)\n",
    "                logits = model(img)\n",
    "        \n",
    "                loss = loss_fn(logits, label)\n",
    "                acc = acc_fn(logits, label)\n",
    "        \n",
    "                losses.append(loss.item())\n",
    "                acces.append(acc.item())\n",
    "        \n",
    "            avg_val_loss = sum(losses) / len(losses)\n",
    "            avg_val_acc = sum(acces) / len(acces)\n",
    "            print(f'In epoch {e}, average validation loss is {avg_val_loss}, average validation acc is {avg_val_acc}.')  \n",
    "\n",
    "        if avg_val_acc > best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            print(f'saving best model with acc {best_val_acc} at {e} epoch!')       \n",
    "            torch.save(model, model_path)\n",
    "\n",
    "        writer.add_scalars('cls/loss', {'train': avg_train_loss, 'val': avg_val_loss}, global_step=e)\n",
    "        writer.add_scalars('cls/acc', {'train': avg_train_acc, 'val': avg_val_acc}, global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae(model, train_dataloader, val_dataloader, writer, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contrastive Learning을 휘한 SimCLR 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. Contrastive Loss 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2) / self.temperature\n",
    "\n",
    "        labels = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool)\n",
    "        labels = labels[~mask].view(labels.shape[0], -1)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "\n",
    "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
    "        negatives = similarity_matrix[~labels.bool()].view(labels.shape[0], -1)\n",
    "\n",
    "        logits = torch.cat([positives, negatives], dim=1)\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)\n",
    "\n",
    "        return F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=2048):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim=128):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.backbone = base_model\n",
    "        self.feature_dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.projection_head = ProjectionHead(self.feature_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection_head(features)\n",
    "\n",
    "class SimCLRClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super(SimCLRClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(encoder.feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder.backbone(x)\n",
    "        return self.fc(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pre-training SimCLR with CIFAR10Dataset (Self-Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. Library & HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCH = 100\n",
    "TEMPERATURE = 0.5\n",
    "OUT_DIM = 128\n",
    "LR = 3e-4\n",
    "\n",
    "model_path = './pth/simclr_pretrained_cifar.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='../../datasets/CIFAR10', train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_simclr(get_data_loaders, model_path):\n",
    "    train_loader = get_data_loaders(BATCH_SIZE)\n",
    "\n",
    "    model = SimCLR(torchvision.models.resnet18(pretrained=False), OUT_DIM).to(device)\n",
    "    criterion = NTXentLoss(temperature=TEMPERATURE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            images_i, images_j = images.to(device), images.to(device)\n",
    "\n",
    "            z_i = model(images_i)\n",
    "            z_j = model(images_j)\n",
    "\n",
    "            loss = criterion(z_i, z_j)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{EPOCH}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_simclr(get_data_loaders, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transfer Learning SimCLR with CIFAR10Dataset (Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1. Library & HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPOCH = 100\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_path = './pth/simclr_classifer_cifar.pt'\n",
    "pretrain_path = './pth/simclr_pretrained_cifar.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation function 정의\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # 이미지 크기를 32x32로 조정\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "    ])\n",
    "    \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "])\n",
    "\n",
    "# 데이터셋 설정\n",
    "train_dataset = torchvision.datasets.CIFAR10('../../datasets/CIFAR10', train=True, download=True, transform=transform_train)\n",
    "val_dataset = torchvision.datasets.CIFAR10('../../datasets/CIFAR10', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "writer = SummaryWriter(os.path.join('logs', 'cifar10', 'mae-pretrain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-3. Network 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr = SimCLR(torchvision.models.resnet18(pretrained=False), OUT_DIM)\n",
    "simclr.load_state_dict(torch.load(pretrain_path))\n",
    "classifier = SimCLRClassifier(simclr, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simclr(classifier, train_loader, val_loader, model_path):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        classifier.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100.0 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{EPOCH}, Loss: {avg_loss:.4f}, Accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    # 모델 평가\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    test_acc = 100.0 * correct / total\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "    torch.save(classifier, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_simclr(classifier, train_loader, val_loader, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-1. Library & HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "OUT_DIM = 128\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASS = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mae_path = './pth/mae_classifer_cifar.pt'\n",
    "simclr_path = './pth/simclr_classifer_cifar.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-2. Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(batch_size, img_size):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "    ])\n",
    "    \n",
    "    test_dataset = torchvision.datasets.CIFAR10('../../datasets/CIFAR10', train=False, download=True, transform=transform_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-3. Compare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE 및 SimCLR 모델 성능 비교\n",
    "def compare_models(mae_path, simclr_path, get_test_loader):\n",
    "    # MAE 모델 로드\n",
    "    mae_classifier = torch.load(mae_path).to(device)\n",
    "\n",
    "    # SimCLR 모델 로드\n",
    "    simclr_classifier = torch.load(simclr_path).to(device)\n",
    "\n",
    "    dataloader = get_test_loader(BATCH_SIZE, IMAGE_SIZE)\n",
    "\n",
    "    results = {\n",
    "        \"MAE\": {\"top1\": 0, \"top5\": 0, \"class_acc\": torch.zeros(NUM_CLASS)},\n",
    "        \"SimCLR\": {\"top1\": 0, \"top5\": 0, \"class_acc\": torch.zeros(NUM_CLASS)}\n",
    "    }\n",
    "\n",
    "    mae_classifier.eval()\n",
    "    simclr_classifier.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # MAE 평가\n",
    "        mae_correct = torch.zeros(NUM_CLASS)\n",
    "        mae_total = torch.zeros(NUM_CLASS)\n",
    "        mae_top1 = 0\n",
    "        mae_top5 = 0\n",
    "\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating MAE\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = mae_classifier(images)\n",
    "\n",
    "            _, predicted = outputs.topk(5, dim=1, largest=True, sorted=True)\n",
    "\n",
    "            mae_top1 += (predicted[:, 0] == labels).sum().item()\n",
    "            mae_top5 += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            c = (predicted[:, 0] == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                mae_correct[label] += c[i].item()\n",
    "                mae_total[label] += 1\n",
    "\n",
    "        results[\"MAE\"][\"top1\"] = 100.0 * mae_top1 / len(dataloader.dataset)\n",
    "        results[\"MAE\"][\"top5\"] = 100.0 * mae_top5 / len(dataloader.dataset)\n",
    "        results[\"MAE\"][\"class_acc\"] = mae_correct / mae_total\n",
    "\n",
    "        # SimCLR 평가\n",
    "        simclr_correct = torch.zeros(NUM_CLASS)\n",
    "        simclr_total = torch.zeros(NUM_CLASS)\n",
    "        simclr_top1 = 0\n",
    "        simclr_top5 = 0\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating SimCLR\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = simclr_classifier(images)\n",
    "\n",
    "            _, predicted = outputs.topk(5, dim=1, largest=True, sorted=True)\n",
    "\n",
    "            simclr_top1 += (predicted[:, 0] == labels).sum().item()\n",
    "            simclr_top5 += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            c = (predicted[:, 0] == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                simclr_correct[label] += c[i].item()\n",
    "                simclr_total[label] += 1\n",
    "\n",
    "        results[\"SimCLR\"][\"top1\"] = 100.0 * simclr_top1 / len(dataloader.dataset)\n",
    "        results[\"SimCLR\"][\"top5\"] = 100.0 * simclr_top5 / len(dataloader.dataset)\n",
    "        results[\"SimCLR\"][\"class_acc\"] = simclr_correct / simclr_total\n",
    "\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(\"MAE - Top-1 Accuracy: {:.2f}%, Top-5 Accuracy: {:.2f}%\".format(results[\"MAE\"][\"top1\"], results[\"MAE\"][\"top5\"]))\n",
    "    print(\"SimCLR - Top-1 Accuracy: {:.2f}%, Top-5 Accuracy: {:.2f}%\".format(results[\"SimCLR\"][\"top1\"], results[\"SimCLR\"][\"top5\"]))\n",
    "\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for label in range(NUM_CLASS):\n",
    "        print(f\"Class {label} - MAE: {results['MAE']['class_acc'][label]:.2f}, SimCLR: {results['SimCLR']['class_acc'][label]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(mae_path, simclr_path, get_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Oxford Dataset에 대해 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-1. Pretrain MAE with OxfordIIIPetDataset (Self-Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 4096\n",
    "MAX_DEVICE_BATCH_SIZE = 512\n",
    "LR = 1.5e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "MASK_RATIO = 0.75\n",
    "EPOCH = 2000\n",
    "WARMUP = 200\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "setup_seed(SEED)\n",
    "\n",
    "model_path = './pth/mae_pretrained_oxford.pt'\n",
    "\n",
    "batch_size = min(MAX_DEVICE_BATCH_SIZE, BATCH_SIZE)\n",
    "assert BATCH_SIZE % batch_size == 0\n",
    "steps_per_update = BATCH_SIZE // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기를 32x32로 조정\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "])\n",
    "\n",
    "# Oxford-IIIT Pet 데이터셋 로딩\n",
    "train_dataset = datasets.OxfordIIITPet(root='../../datasets/OxfordPet', split='trainval', transform=transform, download=True)\n",
    "val_dataset = datasets.OxfordIIITPet(root='../../datasets/OxfordPet', split='test', transform=transform, download=True)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "writer = SummaryWriter(os.path.join('logs', 'oxford-iiit-pet', 'mae-pretrain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAE_ViT(image_size=224, patch_size=16, mask_ratio=MASK_RATIO).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_mae(model, dataloader, val_dataset, writer, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-2.Transfer Learning MAE with OxfordIIIPetDataset (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 128\n",
    "MAX_DEVICE_BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 0.05\n",
    "EPOCH = 100\n",
    "WARMUP = 5\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "setup_seed(SEED)\n",
    "\n",
    "model_path = './pth/mae_classifer_oxford.pt'\n",
    "pretrain_path = './pth/mae_pretrained_oxford.pt'\n",
    "\n",
    "batch_size = min(MAX_DEVICE_BATCH_SIZE, BATCH_SIZE)\n",
    "assert BATCH_SIZE % batch_size == 0\n",
    "steps_per_update = BATCH_SIZE // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.OxfordIIITPet(\n",
    "    root='../../datasets/OxfordPet', split='trainval', target_types='category', transform=transform_train, download=True)\n",
    "val_dataset = datasets.OxfordIIITPet(\n",
    "    root='../../datasets/OxfordPet', split='test', target_types='category', transform=transform_test, download=True)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "writer = SummaryWriter(os.path.join('logs', 'oxford-iiit-pet', 'mae-pretrain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(pretrain_path, map_location='cpu')\n",
    "model = model.to(device)\n",
    "writer = SummaryWriter(os.path.join('logs', 'oxford-iiit-pet', 'pretrain-cls'))\n",
    "\n",
    "model = ViT_Classifier(model.encoder, num_classes=37).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae(model, train_dataloader, val_dataloader, writer, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-3. Pretrain SimCLR with OxfordIIIPetDataset (Self-Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCH = 100\n",
    "TEMPERATURE = 0.5\n",
    "OUT_DIM = 128\n",
    "LR = 3e-4\n",
    "\n",
    "model_path = './pth/simclr_pretrained_oxford.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size):\n",
    "    # 데이터셋 전처리 설정\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # 이미지 크기를 32x32로 조정\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "    ])\n",
    "\n",
    "    # Oxford-IIIT Pet 데이터셋 로딩\n",
    "    train_dataset = torchvision.datasets.OxfordIIITPet(\n",
    "        root='../../datasets/OxfordPet', split='trainval', transform=transform, download=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_simclr(get_data_loaders, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-4.Transfer Learning SimCLR with OxfordIIIPetDataset (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPOCH = 100\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_path = './pth/simclr_classifer_oxford.pt'\n",
    "pretrain_path = './pth/simclr_pretrained_oxford.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Oxford-IIIT Pet 데이터셋 로딩\n",
    "train_dataset = datasets.OxfordIIITPet(\n",
    "    root='../../datasets/OxfordPet', split='trainval', target_types='category', transform=transform_train, download=True)\n",
    "val_dataset = datasets.OxfordIIITPet(\n",
    "    root='../../datasets/OxfordPet', split='test', target_types='category', transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr = SimCLR(torchvision.models.resnet18(pretrained=False), OUT_DIM)\n",
    "simclr.load_state_dict(torch.load(pretrain_path))\n",
    "classifier = SimCLRClassifier(simclr, num_classes=37).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_simclr(classifier, train_loader, val_loader, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-5. Compare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "OUT_DIM = 128\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASS = 37\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mae_path = './pth/mae_classifer_oxford.pt'\n",
    "simclr_path = './pth/simclr_classifer_oxford.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(batch_size, img_size):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 표준 정규화\n",
    "    ])\n",
    "    \n",
    "    test_dataset = datasets.OxfordIIITPet(root='../../datasets/OxfordPet', split='test', transform=transform_test, download=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(mae_path, simclr_path, get_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
