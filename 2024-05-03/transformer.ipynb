{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets if not exist\n",
    "from torchvision.datasets.oxford_iiit_pet import OxfordIIITPet\n",
    "\n",
    "# root\n",
    "root = './data'\n",
    "# download\n",
    "_ = OxfordIIITPet(root, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# nn.Conv2d를 이용한 패치 임베딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Reference\n",
    "# https://github.com/jankrepl/mildlyoverfitted/blob/master/github_adventures/vision_transformer/custom.py\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    embed_dim : int\n",
    "        The emmbedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches\n",
    "        and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        assert img_size % patch_size == 0,\\\n",
    "            f\"img_size({img_size} is not divisable by patch_size({patch_size}))\"\n",
    "\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "# test\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "in_chans = 3\n",
    "embed_dim = 768\n",
    "x = torch.randn(1, in_chans, img_size, img_size)\n",
    "model = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/min/miniconda3/envs/torch2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Transformer based model\n",
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_heads=12, num_layers=12):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2, embed_dim))\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, in_chans * patch_size * patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        memory = self.encoder(x)\n",
    "        x = self.decoder(x, memory)\n",
    "        x = self.proj(x)\n",
    "        x = x.reshape(x.shape[0], in_chans, img_size, img_size)\n",
    "        return x\n",
    "\n",
    "# test\n",
    "model = TransformerAutoEncoder()\n",
    "out = model(torch.randn(1, 3, 224, 224))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# data augmentation\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.uint8),\n",
    "    v2.CenterCrop(size=(IMG_SIZE, IMG_SIZE)),\n",
    "    v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.uint8),\n",
    "    v2.CenterCrop(size=(IMG_SIZE, IMG_SIZE)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# test transforms\n",
    "random_tensor = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n",
    "train_transform_result = train_transforms(random_tensor)\n",
    "test_transform_result = test_transforms(random_tensor)\n",
    "print(train_transform_result.shape)\n",
    "print(test_transform_result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680\n",
      "3669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224]) tensor([13, 22, 14,  5, 12, 10,  3, 15,  6, 35,  2, 22, 30,  5, 12, 24, 10, 28,\n",
      "        19, 26, 35, 10, 31, 19, 23, 16, 23, 12,  1, 36,  6, 12, 36, 26, 14, 18,\n",
      "        31,  3, 25, 27, 29, 33, 21, 19, 16, 26, 22, 29, 34, 28, 31, 14,  0, 28,\n",
      "        22, 11, 13, 20,  9, 24,  0, 32, 21,  1])\n",
      "torch.Size([64, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "# train_datasets\n",
    "train_datasets = OxfordIIITPet(root, transform=train_transforms)\n",
    "test_datasets = OxfordIIITPet(root, split=\"test\", transform=test_transforms)\n",
    "\n",
    "# check datasets length\n",
    "print(len(train_datasets))\n",
    "print(len(test_datasets))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# dataloaders\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=16)\n",
    "test_dataloader = DataLoader(test_datasets, batch_size=BATCH_SIZE, shuffle=False, num_workers=16)\n",
    "\n",
    "# test dataloaders\n",
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y)\n",
    "    break\n",
    "\n",
    "for x, y in test_dataloader:\n",
    "    print(x.shape, y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# 모델 선언\n",
    "model = TransformerAutoEncoder().to(DEVICE)\n",
    "\n",
    "# 옵티마이저, 스케줄러, 손실 함수 선언\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "criterion = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_fn(model, dataloader, criterion, optimizer, scheduler, device, scaler):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, _ in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "def test_fn(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    psnr = 0.0\n",
    "    ssim = 0.0\n",
    "\n",
    "    psnr_metric = torchmetrics.image.PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, images)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            psnr += psnr_metric(outputs, images).item()\n",
    "            ssim += ssim_metric(outputs, images).item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    psnr /= len(dataloader)\n",
    "    ssim /= len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n",
    "    return test_loss\n",
    "\n",
    "def train_autoencoder(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, device, epochs, save_period):\n",
    "    best_loss = float('inf')\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        train_fn(model, train_dataloader, criterion, optimizer, scheduler, device, scaler)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            val_loss = test_fn(model, val_dataloader, criterion, device)\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"best_model_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "        if (epoch + 1) % save_period == 0:\n",
    "            torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 58/58 [00:20<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 58/58 [00:06<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3701, PSNR: 12.21, SSIM: 0.0076\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 4/58 [00:02<00:25,  2.14it/s]"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = train_autoencoder(model, train_dataloader, test_dataloader, criterion, optimizer, scheduler, DEVICE, epochs=100, save_period=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
